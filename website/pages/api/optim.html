
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="module-classy_vision.optim">
<span id="optimizers"></span><h1>Optimizers<a class="headerlink" href="#module-classy_vision.optim" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="classy_vision.optim.ClassyOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">ClassyOptimizer</code><span class="sig-paren">(</span><em class="sig-param">lr_scheduler: classy_vision.optim.param_scheduler.classy_vision_param_scheduler.ClassyParamScheduler</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for classy optimizers.</p>
<p>This wraps a <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.3.0a0+ee77ccb ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance, handles learning
rate scheduling by using a <code class="xref py py-class docutils literal notranslate"><span class="pre">ClassyParamScheduler</span></code> and supports specifying
regularized and unregularized param groups. Specifying unregularized params is
especially useful to avoid applying weight decay on batch norm. See
:method:ClassyModel.get_optimizer_params for more information.</p>
<p>Deriving classes can extend functionality be overriding the appropriate functions.</p>
<p>Constructor for ClassyOptimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lr_scheduler</strong> – The learning rate scheduler to use.</p>
</dd>
</dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss: torch.Tensor</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computer gradients with respect to the loss.</p>
<p>Calls <a href="#id1"><span class="problematic" id="id2">:method:`zero_grad`</span></a> and then computes the gradient using
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.backward()</span></code>. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="(in PyTorch vmaster (1.3.0a0+ee77ccb ))"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> for more information.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.get_classy_state">
<code class="sig-name descname">get_classy_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → Dict[str, Any]<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.get_classy_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the state of the ClassyOptimizer.</p>
<p>The returned state is used for checkpointing.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A state dictionary containing the state of the optimizer.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model: classy_vision.models.classy_model.ClassyModel</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.3.0a0+ee77ccb ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to
super().init_pytorch_optimizer().</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.parameters">
<em class="property">property </em><code class="sig-name descname">parameters</code><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the parameters of the optimizer which need to be overridden. All optimizer
param groups will use these parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A kwarg dictionary that will be used to override optimizer args.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.set_classy_state">
<code class="sig-name descname">set_classy_state</code><span class="sig-paren">(</span><em class="sig-param">state: Dict[str, Any]</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.set_classy_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the state of the ClassyOptimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> – The state dictionary. Must be the output of a call to
<a href="#id3"><span class="problematic" id="id4">:method:`get_classy_state`</span></a>.</p>
</dd>
</dl>
<p>This is used to load the state of the optimizer from a checkpoint.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure: Optional[Callable] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<p>See <a href="#id5"><span class="problematic" id="id6">:method:`torch.optim.Optimizer.step`</span></a> for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> – A closure that re-evaluates the model and returns the loss</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.update_schedule_on_epoch">
<code class="sig-name descname">update_schedule_on_epoch</code><span class="sig-paren">(</span><em class="sig-param">where: float</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.update_schedule_on_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the param schedule at the end of an epoch.</p>
<p>This should be called by the task at the end of every epoch to update the
schedule of epoch based param schedulers (See <code class="xref py py-class docutils literal notranslate"><span class="pre">ClassyParamScheduler</span></code> for
more information).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>where</strong> – where we are in terms of training progress (output of
<a href="#id7"><span class="problematic" id="id8">:method:`ClassyTask.where`</span></a>)</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.update_schedule_on_step">
<code class="sig-name descname">update_schedule_on_step</code><span class="sig-paren">(</span><em class="sig-param">where: float</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.update_schedule_on_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the param schedule at the end of a train step.</p>
<p>This should be called by the task at the end of every train step (
<a href="#id9"><span class="problematic" id="id10">:method:`ClassyTask.train_step`</span></a>) to update the schedule of step based param
schedulers (See <code class="xref py py-class docutils literal notranslate"><span class="pre">ClassyParamScheduler</span></code> for more information).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>where</strong> – where we are in terms of training progress (output of
<a href="#id11"><span class="problematic" id="id12">:method:`ClassyTask.where`</span></a>)</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears the gradients of all optimized parameters.</p>
<p>See <a href="#id13"><span class="problematic" id="id14">:method:`torch.optim.Optimizer.zero_grad`</span></a> for more information.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="classy_vision.optim.RMSProp">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">RMSProp</code><span class="sig-paren">(</span><em class="sig-param">lr_scheduler: classy_vision.optim.param_scheduler.classy_vision_param_scheduler.ClassyParamScheduler</em>, <em class="sig-param">momentum: float</em>, <em class="sig-param">weight_decay: float</em>, <em class="sig-param">alpha: float</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">centered: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.RMSProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for ClassyOptimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lr_scheduler</strong> – The learning rate scheduler to use.</p>
</dd>
</dl>
<dl class="method">
<dt id="classy_vision.optim.RMSProp.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: Dict[str, Any]</em><span class="sig-paren">)</span> → classy_vision.optim.rmsprop.RMSProp<a class="headerlink" href="#classy_vision.optim.RMSProp.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializer for stochastic gradient descent optimizer. The config
is expected to contain at least three keys:</p>
<p>lr: float learning rate
momentum: float momentum (should be [0, 1))
weight_decay: float weight decay</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.RMSProp.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.RMSProp.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.3.0a0+ee77ccb ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to
super().init_pytorch_optimizer().</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.RMSProp.parameters">
<em class="property">property </em><code class="sig-name descname">parameters</code><a class="headerlink" href="#classy_vision.optim.RMSProp.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the parameters of the optimizer which need to be overridden. All optimizer
param groups will use these parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A kwarg dictionary that will be used to override optimizer args.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="classy_vision.optim.SGD">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">SGD</code><span class="sig-paren">(</span><em class="sig-param">lr_scheduler</em>, <em class="sig-param">momentum</em>, <em class="sig-param">weight_decay</em>, <em class="sig-param">nesterov=False</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for ClassyOptimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lr_scheduler</strong> – The learning rate scheduler to use.</p>
</dd>
</dl>
<dl class="method">
<dt id="classy_vision.optim.SGD.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.SGD.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializer for stochastic gradient descent optimizer. The config
is expected to contain at least three keys:</p>
<p>lr: float learning rate
momentum: float momentum (should be [0, 1))
weight_decay: float weight decay</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.SGD.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.SGD.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.3.0a0+ee77ccb ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to
super().init_pytorch_optimizer().</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.SGD.parameters">
<em class="property">property </em><code class="sig-name descname">parameters</code><a class="headerlink" href="#classy_vision.optim.SGD.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the parameters of the optimizer which need to be overridden. All optimizer
param groups will use these parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A kwarg dictionary that will be used to override optimizer args.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Classy Vision</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Library Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="heads.html">Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="hooks.html">Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="meters.html">Meters</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tasks.html">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="index.html">Documentation overview</a><ul>
<li>Previous: <a href="models.html" title="previous chapter">Models</a></li>
<li>Next: <a href="tasks.html" title="next chapter">Tasks</a></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>