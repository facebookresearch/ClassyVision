<!DOCTYPE html><html lang=""><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Classy Vision · An end-to-end framework for image and video classification</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="An end-to-end framework for image and video classification"/><meta property="og:title" content="Classy Vision · An end-to-end framework for image and video classification"/><meta property="og:type" content="website"/><meta property="og:url" content="https://classyvision.ai/"/><meta property="og:description" content="An end-to-end framework for image and video classification"/><meta property="og:image" content="https://classyvision.ai/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://classyvision.ai/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="stylesheet" href="/css/code_block_buttons.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/code_block_buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon.png" alt="Classy Vision"/><h2 class="headerTitleWithLogo">Classy Vision</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/facebookresearch/ClassyVision" target="_self">GitHub</a></li><li class=""><a target="_self"></a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span></span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Tutorials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/tutorials/">Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Using Classy Vision</h3><ul class=""><li class="navListItem"><a class="navItem" href="/tutorials/getting_started">Getting started</a></li><li class="navListItem"><a class="navItem" href="/tutorials/ray_aws">Distributed training on AWS</a></li><li class="navListItem"><a class="navItem" href="/tutorials/classy_dataset">Creating a custom dataset</a></li><li class="navListItem"><a class="navItem" href="/tutorials/classy_model">Creating and using Classy Models</a></li><li class="navListItem"><a class="navItem" href="/tutorials/classy_loss">Creating a custom loss</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/tutorials/video_classification">How to do video classification</a></li><li class="navListItem"><a class="navItem" href="/tutorials/fine_tuning">Fine Tuning a model</a></li><li class="navListItem"><a class="navItem" href="/tutorials/pet_aws">Elastic training</a></li><li class="navListItem"><a class="navItem" href="/tutorials/torchscript">Productionizing models</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="tutorialBody">
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js">
</script>
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js">
</script>
<body class="notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="How-to-do-video-classification">How to do video classification<a class="anchor-link" href="#How-to-do-video-classification">¶</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>In this tutorial, we will show how to train a video classification model in Classy Vision. Given an input video, the video classification task is to predict the most probable class label. This is very similar to image classification, which was covered in other tutorials, but there are a few differences that make video special. As the video duration can be long, we sample short video clips of a small number of frames, use the classifier to make predictions, and finally average the clip-level predictions to get the final video-level predictions.</p>
<p>In this tutorial we will: (1) load a video dataset; (2) configure a video model; (3) configure video meters; (4) build a task; (5) start training; Please note that these steps are being done separately in the tutorial for easy of exposition in the notebook format. As described in our <a href="https://classyvision.ai/tutorials/getting_started">Getting started</a> tutorial, you can combine all configs used in this tutorial into a single config for ClassificationTask and train it using <code>classy_train.py</code>.</p>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Before we get started, let us enable INFO level logging so that we can monitor the progress of our runs.</p>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="1.-Prepare-the-dataset">1. Prepare the dataset<a class="anchor-link" href="#1.-Prepare-the-dataset">¶</a></h2><p>All right! Let's start with the dataset. <a href="https://www.crcv.ucf.edu/data/UCF101.php">UCF-101</a> is a canonical action recognition dataset. It has 101 action classes, and has 3 folds with different training/testing splitting . We use fold 1 in this tutorial. Classy Vision has implemented the dataset <code>ucf101</code>, which can be used to load the training and testing splits.</p>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="1.1-Directories-and-Metadata-File-information">1.1 Directories and Metadata File information<a class="anchor-link" href="#1.1-Directories-and-Metadata-File-information">¶</a></h3><p>You will need to download the videos and the split files of UCF-101 dataset from the <a href="https://www.crcv.ucf.edu/data/UCF101.php">official site</a>.</p>
<p>You should then have the videos present in a folder -</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> ls /path/to/ucf101
<span class="go">ApplyEyeMakeup</span>
<span class="go">...</span>
<span class="go">YoYo</span>
</pre></div>
<p>There also needs to be a folder which contains the split files of the dataset -</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> ls /path/to/UCF101TrainTestSplits-RecognitionTask
<span class="go">classInd.txt</span>
<span class="go">...</span>
<span class="go">trainlist03.txt</span>
</pre></div>
<p>Upon initializing the dataset, Classy Vision processes all this dataset information and stores it in a metadata file. This metadata file can be re-used for future runs to make the initialization faster. You can pass the path to store the metadata as <code>/path/to/ucf101_metadata.pt</code>.</p>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">classy_vision.dataset</span> <span class="kn">import</span> <span class="n">build_dataset</span>

<span class="c1"># set it to the folder where video files are saved</span>
<span class="n">video_dir</span> <span class="o">=</span> <span class="s2">"/path/to/ucf101"</span>
<span class="c1"># set it to the folder where dataset splitting files are saved</span>
<span class="n">splits_dir</span> <span class="o">=</span> <span class="s2">"/path/to/UCF101TrainTestSplits-RecognitionTask"</span>
<span class="c1"># set it to the file path for saving the metadata</span>
<span class="n">metadata_file</span> <span class="o">=</span> <span class="s2">"/path/to/ucf101_metadata.pt"</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">datasets</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">({</span>
    <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"ucf101"</span><span class="p">,</span>
    <span class="s2">"split"</span><span class="p">:</span> <span class="s2">"train"</span><span class="p">,</span>
    <span class="s2">"batchsize_per_replica"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># For training, we use 8 clips in a minibatch in each model replica</span>
    <span class="s2">"use_shuffle"</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>         <span class="c1"># We shuffle the clips in the training split</span>
    <span class="s2">"num_samples"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>           <span class="c1"># We train on 16 clips in one training epoch</span>
    <span class="s2">"clips_per_video"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># For training, we randomly sample 1 clip from each video</span>
    <span class="s2">"frames_per_clip"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>        <span class="c1"># The video clip contains 8 frames</span>
    <span class="s2">"video_dir"</span><span class="p">:</span> <span class="n">video_dir</span><span class="p">,</span>
    <span class="s2">"splits_dir"</span><span class="p">:</span> <span class="n">splits_dir</span><span class="p">,</span>
    <span class="s2">"metadata_file"</span><span class="p">:</span> <span class="n">metadata_file</span><span class="p">,</span>
    <span class="s2">"fold"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"transforms"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"video"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"video_default_augment"</span><span class="p">,</span>
                <span class="s2">"crop_size"</span><span class="p">:</span> <span class="mi">112</span><span class="p">,</span>
                <span class="s2">"size_range"</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">160</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">})</span>
<span class="n">datasets</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">({</span>
    <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"ucf101"</span><span class="p">,</span>
    <span class="s2">"split"</span><span class="p">:</span> <span class="s2">"test"</span><span class="p">,</span>
    <span class="s2">"batchsize_per_replica"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>  <span class="c1"># For testing, we will take 1 video once a time, and sample 10 clips per video</span>
    <span class="s2">"use_shuffle"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>         <span class="c1"># We do not shuffle clips in the testing split</span>
    <span class="s2">"num_samples"</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>            <span class="c1"># We test on 80 clips in one testing epoch</span>
    <span class="s2">"clips_per_video"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>        <span class="c1"># We sample 10 clips per video</span>
    <span class="s2">"frames_per_clip"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s2">"video_dir"</span><span class="p">:</span> <span class="n">video_dir</span><span class="p">,</span>
    <span class="s2">"splits_dir"</span><span class="p">:</span> <span class="n">splits_dir</span><span class="p">,</span>
    <span class="s2">"metadata_file"</span><span class="p">:</span> <span class="n">metadata_file</span><span class="p">,</span>
    <span class="s2">"fold"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"transforms"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"video"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"video_default_no_augment"</span><span class="p">,</span>
                <span class="s2">"size"</span><span class="p">:</span> <span class="mi">128</span>
            <span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>    
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Note we specify different transforms for training and testing split. For training split, we first randomly select a size from <code>size_range</code> [128, 160], and resize the video clip so that its short edge is equal to the random size. After that, we take a random crop of spatial size 112 x 112. We find such data augmentation helps the model generalize better, and use it as the default transform with data augmentation. For testing split, we resize the video clip to have short edge of size 128, and skip the random cropping to use the entire video clip. This is the default transform without data augmentation.</p>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="2.-Define-a-model-trunk-and-a-head">2. Define a model trunk and a head<a class="anchor-link" href="#2.-Define-a-model-trunk-and-a-head">¶</a></h2><p>Next, let's create the video model, which consists of a trunk and a head. The trunk can be viewed as a feature extractor for computing discriminative features from raw video pixels while the head is viewed as a classifier for producing the final predictions. Let's first create the trunk of architecture ResNet3D-18 by using the built-in <code>resnext3d</code> model in Classy Vision.</p>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">classy_vision.models</span> <span class="kn">import</span> <span class="n">build_model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">({</span>
    <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"resnext3d"</span><span class="p">,</span>
    <span class="s2">"frames_per_clip"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>        <span class="c1"># The number of frames we have in each video clip</span>
    <span class="s2">"input_planes"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>           <span class="c1"># We use RGB video frames. So the input planes is 3</span>
    <span class="s2">"clip_crop_size"</span><span class="p">:</span> <span class="mi">112</span><span class="p">,</span>       <span class="c1"># We take croppings of size 112 x 112 from the video frames </span>
    <span class="s2">"skip_transformation_type"</span><span class="p">:</span> <span class="s2">"postactivated_shortcut"</span><span class="p">,</span>    <span class="c1"># The type of skip connection in residual unit</span>
    <span class="s2">"residual_transformation_type"</span><span class="p">:</span> <span class="s2">"basic_transformation"</span><span class="p">,</span>  <span class="c1"># The type of residual connection in residual unit</span>
    <span class="s2">"num_blocks"</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>  <span class="c1"># The number of residual blocks in each of the 4 stages </span>
    <span class="s2">"input_key"</span><span class="p">:</span> <span class="s2">"video"</span><span class="p">,</span>        <span class="c1"># The key used to index into the model input of dict type </span>
    <span class="s2">"stage_planes"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>    
    <span class="s2">"num_classes"</span><span class="p">:</span> <span class="mi">101</span>           <span class="c1"># the number of classes</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>We also need to create a model head, which consists of an average pooling layer and a linear layer, by using the <code>fully_convolutional_linear</code> head. At test time, the shape (channels, frames, height, width) of input tensor is typically <code>(3 x 8 x 128 x 173)</code>. The shape of input tensor to the average pooling layer is <code>(512, 1, 8, 10)</code>. Since we do not use a global average pooling but an average pooling layer of kernel size <code>(1, 7, 7)</code>, the pooled feature map has shape <code>(512, 1, 2, 5)</code>. The shape of prediction tensor from the linear layer is <code>(1, 2, 5, 101)</code>, which indicates the model computes a 101-D prediction vector densely over a <code>2 x 5</code> grid. That's why we name the head as <code>FullyConvolutionalLinearHead</code> because we use the linear layer as a <code>1x1</code> convolution layer to produce spatially dense predictions. Finally, predictions over the <code>2 x 5</code> grid are averaged.</p>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">classy_vision.heads</span> <span class="kn">import</span> <span class="n">build_head</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="n">unique_id</span> <span class="o">=</span> <span class="s2">"default_head"</span>
<span class="n">head</span> <span class="o">=</span> <span class="n">build_head</span><span class="p">({</span>
    <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"fully_convolutional_linear"</span><span class="p">,</span>
    <span class="s2">"unique_id"</span><span class="p">:</span> <span class="n">unique_id</span><span class="p">,</span>
    <span class="s2">"pool_size"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
    <span class="s2">"num_classes"</span><span class="p">:</span> <span class="mi">101</span><span class="p">,</span>
    <span class="s2">"in_plane"</span><span class="p">:</span> <span class="mi">512</span>    
<span class="p">})</span>
<span class="c1"># In Classy Vision, the head can be attached to any residual block in the trunk. </span>
<span class="c1"># Here we attach the head to the last block as in the standard ResNet model</span>
<span class="n">fork_block</span> <span class="o">=</span> <span class="s2">"pathway0-stage4-block1"</span>
<span class="n">heads</span> <span class="o">=</span> <span class="p">{</span><span class="n">fork_block</span><span class="p">:</span> <span class="p">[</span><span class="n">head</span><span class="p">]}</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_heads</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="3.-Choose-the-meters">3. Choose the meters<a class="anchor-link" href="#3.-Choose-the-meters">¶</a></h2><p>This is the biggest difference between video and image classification. For images we used <code>AccuracyMeter</code> to measure top-1 and top-5 accuracy. For videos you can also use both <code>AccuracyMeter</code> and <code>VideoAccuracyMeter</code>, but they behave differently:</p>
<ul>
<li><code>AccuracyMeter</code> takes one clip-level prediction and compare it with groundtruth video label. It reports the clip-level accuracy.</li>
<li><p><code>VideoAccuracyMeter</code> takes multiple clip-level predictions from the same video, averages them and compares that with groundtruth video label. It reports the video-level accuracy which is usually higher than clip-level accuracy.</p>
<p>Both meters report top-1 and top-5 accuracy.</p>
</li>
</ul>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">classy_vision.meters</span> <span class="kn">import</span> <span class="n">build_meters</span><span class="p">,</span> <span class="n">AccuracyMeter</span><span class="p">,</span> <span class="n">VideoAccuracyMeter</span>

<span class="n">meters</span> <span class="o">=</span> <span class="n">build_meters</span><span class="p">({</span>
    <span class="s2">"accuracy"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"topk"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s2">"video_accuracy"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"topk"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
        <span class="s2">"clips_per_video_train"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">"clips_per_video_test"</span><span class="p">:</span> <span class="mi">10</span>
    <span class="p">}</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="4.-Build-a-task">4. Build a task<a class="anchor-link" href="#4.-Build-a-task">¶</a></h2><p>Great! we have defined the minimal set of components necessary for video classification, including dataset, model, loss function, meters and optimizer. We proceed to define a video classification task, and populate it with all the components.</p>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">classy_vision.tasks</span> <span class="kn">import</span> <span class="n">ClassificationTask</span>
<span class="kn">from</span> <span class="nn">classy_vision.optim</span> <span class="kn">import</span> <span class="n">build_optimizer</span>
<span class="kn">from</span> <span class="nn">classy_vision.losses</span> <span class="kn">import</span> <span class="n">build_loss</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">build_loss</span><span class="p">({</span><span class="s2">"name"</span><span class="p">:</span> <span class="s2">"CrossEntropyLoss"</span><span class="p">})</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">build_optimizer</span><span class="p">({</span>
    <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"sgd"</span><span class="p">,</span>
    <span class="s2">"param_schedulers"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"lr"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"multistep"</span><span class="p">,</span>
            <span class="s2">"values"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">],</span>
            <span class="s2">"milestones"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="s2">"num_epochs"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"weight_decay"</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span> <span class="mf">0.9</span>
<span class="p">})</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">task</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">ClassificationTask</span><span class="p">()</span>
    <span class="o">.</span><span class="n">set_num_epochs</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>
    <span class="o">.</span><span class="n">set_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="o">.</span><span class="n">set_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="o">.</span><span class="n">set_meters</span><span class="p">(</span><span class="n">meters</span><span class="p">)</span>
<span class="p">)</span> 
<span class="k">for</span> <span class="n">phase</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"train"</span><span class="p">,</span> <span class="s2">"test"</span><span class="p">]:</span>
    <span class="n">task</span><span class="o">.</span><span class="n">set_dataset</span><span class="p">(</span><span class="n">datasets</span><span class="p">[</span><span class="n">phase</span><span class="p">],</span> <span class="n">phase</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="5.-Start-training">5. Start training<a class="anchor-link" href="#5.-Start-training">¶</a></h2><p>After creating a task, you can simply pass that to a Trainer to start training. Here we will train on a single node and 
configure logging and checkpoints for training:</p>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="CodeMirror cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">classy_vision.trainer</span> <span class="kn">import</span> <span class="n">LocalTrainer</span>
<span class="kn">from</span> <span class="nn">classy_vision.hooks</span> <span class="kn">import</span> <span class="n">CheckpointHook</span>
<span class="kn">from</span> <span class="nn">classy_vision.hooks</span> <span class="kn">import</span> <span class="n">LossLrMeterLoggingHook</span>

<span class="n">hooks</span> <span class="o">=</span> <span class="p">[</span><span class="n">LossLrMeterLoggingHook</span><span class="p">(</span><span class="n">log_freq</span><span class="o">=</span><span class="mi">4</span><span class="p">)]</span>

<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"/tmp/classy_checkpoint_</span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
<span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">CheckpointHook</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">input_args</span><span class="o">=</span><span class="p">{}))</span>

<span class="n">task</span> <span class="o">=</span> <span class="n">task</span><span class="o">.</span><span class="n">set_hooks</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">LocalTrainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>As the training progresses, you should see <code>LossLrMeterLoggingHook</code> printing the loss, learning rate and meter metrics. Checkpoints will be available in the folder created above.</p>
<h2 id="6.-Conclusion">6. Conclusion<a class="anchor-link" href="#6.-Conclusion">¶</a></h2><p>Video classification is very similar to image classification in Classy Vision, you just need to use an appropriate dataset, model and meters. This tutorial glossed over many details about training, please take a look at our <a href="https://classyvision.ai/tutorials/getting_started">Getting started</a> tutorial to learn more. Refer to our API reference for more details about <a href="https://classyvision.ai/api/models.html#classy_vision.models.ResNeXt3D">ResNeXt3D</a> models, <a href="https://classyvision.ai/api/dataset.html#classy_vision.dataset.UCF101Dataset">UCF101</a> dataset and <a href="http://classyvision.ai/api/meters.html#classy_vision.meters.VideoAccuracyMeter">VideoAccuracy</a> meters.</p>
</div>
</div>
</body></div><div class="tutorialButtonWrapper buttonWrapper"><a class="tutorialButton button" download="" href="/files/video_classification.ipynb" target="_blank"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-download" class="svg-inline--fa fa-file-download fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M224 136V0H24C10.7 0 0 10.7 0 24v464c0 13.3 10.7 24 24 24h336c13.3 0 24-10.7 24-24V160H248c-13.2 0-24-10.8-24-24zm76.45 211.36l-96.42 95.7c-6.65 6.61-17.39 6.61-24.04 0l-96.42-95.7C73.42 337.29 80.54 320 94.82 320H160v-80c0-8.84 7.16-16 16-16h32c8.84 0 16 7.16 16 16v80h65.18c14.28 0 21.4 17.29 11.27 27.36zM377 105L279.1 7c-4.5-4.5-10.6-7-17-7H256v128h128v-6.1c0-6.3-2.5-12.4-7-16.9z"></path></svg>Download Tutorial Jupyter Notebook</a></div><div class="tutorialButtonWrapper buttonWrapper"><a class="tutorialButton button" download="" href="/files/video_classification.py" target="_blank"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-download" class="svg-inline--fa fa-file-download fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M224 136V0H24C10.7 0 0 10.7 0 24v464c0 13.3 10.7 24 24 24h336c13.3 0 24-10.7 24-24V160H248c-13.2 0-24-10.8-24-24zm76.45 211.36l-96.42 95.7c-6.65 6.61-17.39 6.61-24.04 0l-96.42-95.7C73.42 337.29 80.54 320 94.82 320H160v-80c0-8.84 7.16-16 16-16h32c8.84 0 16 7.16 16 16v80h65.18c14.28 0 21.4 17.29 11.27 27.36zM377 105L279.1 7c-4.5-4.5-10.6-7-17-7H256v128h128v-6.1c0-6.3-2.5-12.4-7-16.9z"></path></svg>Download Tutorial Source Code</a></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><h5>Documentation</h5><a href="/tutorials/">Tutorials</a><a href="/api/">API Reference</a></div><div class="footerSection"><h5>Social</h5><div class="social"><a class="github-button" href="https://github.com/facebookresearch/ClassyVision" data-count-href="https://github.com/facebookresearch/ClassyVision/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star Classy Vision on GitHub">ClassyVision</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright"><span>Copyright © 2020 Facebook Inc.</span> </section><script>
            (function() {
              var BAD_BASE = '/classyvision/';
              if (window.location.origin !== 'https://classyvision.ai') {
                var pathname = window.location.pathname;
                var newPathname = pathname.slice(pathname.indexOf(BAD_BASE) === 0 ? BAD_BASE.length : 1);
                var newLocation = 'https://classyvision.ai/' + newPathname;
                console.log('redirecting to ' + newLocation);
                window.location.href = newLocation;
              }
            })();
          </script></footer></div></body></html>