{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic training with Classy Vision\n",
    "\n",
    "This tutorial will demonstrate how to launch an training job on Amazon Web Services (AWS) using [PyTorch Elastic](https://github.com/pytorch/elastic) and Classy Vision.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Familiarity with basic AWS (EC2, Auto Scaling Groups, S3, EFS).\n",
    "2. (suggested) install and setup [`awscli`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html).\n",
    "3. Basic knowledge of containers (we use Docker in our examples).\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "Install boto3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download PyTorch Elastic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/pytorch/elastic.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are familiar with the following AWS resources:\n",
    "\n",
    "  1. EC2 [instance profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)\n",
    "  2. EC2 [key pair](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html)\n",
    "  3. [Subnet(s)](https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html#create-default-subnet)\n",
    "  4. [Security group](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#DefaultSecurityGroup)\n",
    "  5. EFS volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create specs file\n",
    "\n",
    "`petctl` is a commandline tool that helps run distributed jobs written with torchelastic on EC2 instances. `petctl` reads a configuration file and sets up a cluster on AWS matching a given configuration. The next sections will describe how to write a configuration file for your needs. \n",
    "\n",
    "First, lets create a launch spec. This is a simple json file that specifies\n",
    "the launch configuration of EC2 instances. We have included a\n",
    "[sample specs file](config/sample_specs.json) so make a copy and fill it in.\n",
    "You only need to fill in the fields with `<YOUR XXXXX>`, you can leave the other\n",
    "fields alone for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd $torchelastic_repository_root\n",
    "! mkdir ~/torchelastic_workspace\n",
    "! cp aws/config/sample_specs.json ~/torchelastic_workspace/specs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specs file is divided into two sections: `rdzv` and `worker`. As their names\n",
    "imply the `rdzv` section contains the launch specs for the instances\n",
    "of the rendezvous backend (e.g. etcd). The `worker` section contains the launch\n",
    "specs for the worker instances.\n",
    "\n",
    "The following subsections describe the fields in the specs file.\n",
    "\n",
    "#### Instance Type and Accelerator\n",
    "````\n",
    "    \"instance_type\" : \"[ec2 instance type]\",\n",
    "    \"accelerator\" : \"[none | gpu]\",\n",
    "````\n",
    "The instance type specifies the EC2 instance type to use. The `accelerator`\n",
    "field can either be `none` or `gpu`. If an EC2 instance that has GPU capability\n",
    "is specified (e.g. `g3`, `p2`, `p3` instance families) then you must specify\n",
    "`accelerator = gpu`. \n",
    "\n",
    "> If `accelerator=gpu` is not specified on a GPU capable instance type,\n",
    "`petctl` assumes you will only use CPU and will use an AMI that does not have\n",
    "CUDA nor NVIDIA drivers.\n",
    "\n",
    "#### Subnet\n",
    "Note that you can add multiple subnets. Each subnet belongs to an availability zone (AZ)\n",
    "so you can spread your instances across AZs by specifying multiple subnets:\n",
    "\n",
    "```\n",
    "    \"subnets\" : [\n",
    "      \"[subnet_in_us-west-2a]\",\n",
    "      \"[subnet_in_us-west-2b]\",\n",
    "      ...\n",
    "    ],\n",
    "```\n",
    "> Some instances are not available in all AZs, make sure to create the subnet\n",
    "in the AZ that supports the instance type that you plan to run your jobs on.\n",
    "\n",
    "#### Security Group\n",
    "torchelastic jobs are distributed and hence require nodes to communicate with each\n",
    "other. Make sure that your security group allows all inbound traffic between\n",
    "instances within the same security group.\n",
    "> Optionally you may want to allow inbound SSH access in case you need to log\n",
    "into the instance for debugging.\n",
    "\n",
    "#### EC2 Instance Profile\n",
    "```\n",
    "    \"instance_role\" : \"[ec2 instance profile]\",\n",
    "```\n",
    "This is the IAM role that is used when accessing other AWS services from **within**\n",
    "the EC2 instance (e.g. accessing S3 from the worker host). To learn more about\n",
    "instance profiles refer to the\n",
    "AWS [documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html).\n",
    "\n",
    "For this example we will require S3 Read Only access from the workers since\n",
    "we will use S3 to upload your local script and download it on the worker-side.\n",
    "\n",
    "> You may wish to add other privileges to this role depending on what your workers\n",
    "do. For instance, if your job writes output to S3, then you will have to attach\n",
    "S3 Write IAM policy to this profile.\n",
    "\n",
    "#### S3 Bucket and Prefix\n",
    "`petctl` uploads your local script to S3 and pulls it down on the worker.\n",
    "Specify the S3 bucket and prefix for this purpose:\n",
    "```\n",
    "    \"s3_bucket\" : \"<YOUR S3 BUCKET NAME>\",\n",
    "    \"s3_prefix\" : \"<YOUR S3 PREFIX>\"\n",
    "```\n",
    "\n",
    "#### Additional Worker Specs\n",
    "Workers have a couple of additional specs compared to rdzv.\n",
    "\n",
    "##### Docker Image\n",
    "```\n",
    "\"docker_image\" : \"torchelastic/aws:0.1.0-rc\",\n",
    "```\n",
    "\n",
    "Note that the `worker` section in the specs file has an extra `docker_image`\n",
    "configuration. This is because the workers run in a docker container whereas\n",
    "the rendezvous backend (etcd) runs directly on the instance. The\n",
    "`torchelastic/aws` image contains torchelastic (along with all of its dependencies)\n",
    "and a few runtime utilities such as the `fetch_and_run` script that allows us\n",
    "to run arbitrary user scripts. For production, you may consider creating your\n",
    "own docker image with a custom `ENTRYPOINT` specific to your application.\n",
    "\n",
    "##### EFS\n",
    "An Elastic File System volume is mounted on each\n",
    "worker instance (it is mounted **through** all the way to the container).\n",
    "EFS acts much like NFS in terms of semantics. Use it as if you were using NFS.\n",
    "You may store your input dataset, store model checkpoints, or job outputs here.\n",
    "\n",
    "> The specified EFS volume is mounted on `/mnt/efs1`. On the host and container.\n",
    "\n",
    "\n",
    "## 3. Create a classy vision project\n",
    "This is as simple as running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! classy-project my-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a look at our [Getting started](https://classyvision.ai/tutorials/getting_started) for more information about how to create a project.\n",
    "\n",
    "\n",
    "## 4. Start training\n",
    "\n",
    "Normally you would run classy_train.py directly to start training. For elastic training, we'll use `petctl` to launch `classy_train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! SPECS_FILE=~/torchelastic_workspace/specs.json python3 petctl.py run_job --size 2 --specs_file ${SPECS_FILE} --name ${user}-job ./classy_train.py -- --config configs/template_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the named arguments, such as, `--size` and `--specs_file` are \n",
    "self explanatory and are arguments supplied to `petctl`. The positional arguments have the form:\n",
    "\n",
    "```\n",
    "[local script] -- [script args ...]\n",
    "  -- or -- \n",
    "[local directory] [script] -- [script args...]\n",
    "```\n",
    "\n",
    "If the first positional argument is a path to a script file, then the script\n",
    "is uploaded to S3 and the script arguments specified after the `--` delimiter\n",
    "are passed through to the script.\n",
    "\n",
    "If the first positional argument is a directory, then a tarball of the directory\n",
    "is created and uploaded to S3 and is extracted on the worker-side. In this case\n",
    "the second positional argument is the path to the script **relative** to the\n",
    "specified directory and, as before, the arguments that follow `--` are passed\n",
    "through to the script.\n",
    "\n",
    "\n",
    "(TIP) Besides a local script or directory you can run with scripts or `tar` files\n",
    "that have already been uploaded to S3 or directly point it to a file or directory\n",
    "on the container.\n",
    "``` bash\n",
    "python3 petctl.py run_job [...] s3://my-bucket/my_script.py\n",
    "python3 petctl.py run_job [...] s3://my-bucket/my_dir.tar.gz my_script.py\n",
    "\n",
    "# or\n",
    "python3 petctl.py run_job [...] --no_upload /path/in/container/dir my_script.py\n",
    "python3 petctl.py run_job [...] --no_upload /path/in/container/dir/my_script.py\n",
    "```\n",
    "\n",
    "Once the `run_job` command returns log into the EC2 console, you will see two\n",
    "Auto Scaling Groups\n",
    "1. etcd server \n",
    "2. workers\n",
    "\n",
    "The workers run in a docker container. You can take a look at their console outputs by running\n",
    "\n",
    "``` bash\n",
    "# get the container id\n",
    "docker ps\n",
    "# tail the container logs\n",
    "docker logs -f <container id>\n",
    "```\n",
    "> Note: by design, `petctl` tries to use the least number of AWS services. This\n",
    "was done intentionally to allow non-AWS users to easily transfer the functionality\n",
    "to their environment. Hence it currently does not have the functionality to query\n",
    "status of the job or to terminate the ASG when the job is done (there is nothing\n",
    "that is monitoring the job!). In practice consider using EKS, Batch, or SageMaker.\n",
    "\n",
    "## 5. Finish training\n",
    "To stop the job and tear down the resources, use the `kill_job` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 petctl.py kill_job --name ${user}-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the two ASGs created with the `run_job` command are deleted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
