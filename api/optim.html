<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Classy Vision · An end-to-end framework for image and video classification</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="An end-to-end framework for image and video classification"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Classy Vision · An end-to-end framework for image and video classification"/><meta property="og:type" content="website"/><meta property="og:url" content="https://classyvision.ai/"/><meta property="og:description" content="An end-to-end framework for image and video classification"/><meta property="og:image" content="https://classyvision.ai/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://classyvision.ai/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="stylesheet" href="/css/code_block_buttons.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/code_block_buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon.png" alt="Classy Vision"/><h2 class="headerTitleWithLogo">Classy Vision</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/facebookresearch/ClassyVision" target="_self">GitHub</a></li><li class=""><a target="_self"></a></li></ul></nav></div></header></div></div><div class="navPusher"><div>
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="module-classy_vision.optim">
<span id="optimizers"></span><h1>Optimizers<a class="headerlink" href="#module-classy_vision.optim" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="classy_vision.optim.Adam">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">Adam</code><span class="sig-paren">(</span><em class="sig-param">lr: float = 0.1</em>, <em class="sig-param">betas: Tuple[float</em>, <em class="sig-param">float] = (0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">amsgrad: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.Adam" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="classy_vision.optim.Adam.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">lr: float = 0.1</em>, <em class="sig-param">betas: Tuple[float</em>, <em class="sig-param">float] = (0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">amsgrad: bool = False</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.Adam.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for ClassyOptimizer.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.Adam.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: Dict[str, Any]</em><span class="sig-paren">)</span> → classy_vision.optim.adam.Adam<a class="headerlink" href="#classy_vision.optim.Adam.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a Adam from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> – A configuration for a Adam.
See <a class="reference internal" href="#classy_vision.optim.Adam.__init__" title="classy_vision.optim.Adam.__init__"><code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code></a> for parameters expected in the config.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Adam instance.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.Adam.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.Adam.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.4.0a0+ac9da29 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to</p>
<p><code class="docutils literal notranslate"><span class="pre">super().init_pytorch_optimizer()</span></code></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="classy_vision.optim.ClassyOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">ClassyOptimizer</code><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for classy optimizers.</p>
<p>This wraps a <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.4.0a0+ac9da29 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance, handles learning
rate scheduling by using a <a class="reference internal" href="param_scheduler.html#classy_vision.optim.param_scheduler.ClassyParamScheduler" title="classy_vision.optim.param_scheduler.ClassyParamScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">param_scheduler.ClassyParamScheduler</span></code></a>
and supports specifying regularized and unregularized param groups.
Specifying unregularized params is especially useful to avoid applying
weight decay on batch norm. See
<a class="reference internal" href="models.html#classy_vision.models.ClassyModel.get_optimizer_params" title="classy_vision.models.ClassyModel.get_optimizer_params"><code class="xref py py-func docutils literal notranslate"><span class="pre">classy_vision.models.ClassyModel.get_optimizer_params()</span></code></a> for more
information.</p>
<p>Deriving classes can extend functionality be overriding the appropriate functions.</p>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for ClassyOptimizer.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss: torch.Tensor</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computer gradients with respect to the loss.</p>
<p>Calls <a class="reference internal" href="#classy_vision.optim.ClassyOptimizer.zero_grad" title="classy_vision.optim.ClassyOptimizer.zero_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">zero_grad()</span></code></a> and then computes the gradient using
<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.backward">torch.Tensor.backward</a>. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="(in PyTorch vmaster (1.4.0a0+ac9da29 ))"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> for
more information.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: Dict[str, Any]</em><span class="sig-paren">)</span> → classy_vision.optim.classy_optimizer.ClassyOptimizer<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a ClassyOptimizer from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> – A configuration for the ClassyOptimizer.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A ClassyOptimizer instance.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.get_classy_state">
<code class="sig-name descname">get_classy_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → Dict[str, Any]<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.get_classy_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the state of the ClassyOptimizer.</p>
<p>The returned state is used for checkpointing.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A state dictionary containing the state of the optimizer.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model: classy_vision.models.classy_model.ClassyModel</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.4.0a0+ac9da29 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to</p>
<p><code class="docutils literal notranslate"><span class="pre">super().init_pytorch_optimizer()</span></code></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.set_classy_state">
<code class="sig-name descname">set_classy_state</code><span class="sig-paren">(</span><em class="sig-param">state: Dict[str, Any]</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.set_classy_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the state of the ClassyOptimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> – The state dictionary. Must be the output of a call to
<a class="reference internal" href="#classy_vision.optim.ClassyOptimizer.get_classy_state" title="classy_vision.optim.ClassyOptimizer.get_classy_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_classy_state()</span></code></a>.</p>
</dd>
</dl>
<p>This is used to load the state of the optimizer from a checkpoint.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.set_param_schedulers">
<code class="sig-name descname">set_param_schedulers</code><span class="sig-paren">(</span><em class="sig-param">param_schedulers: Dict[str, classy_vision.optim.param_scheduler.classy_vision_param_scheduler.ClassyParamScheduler]</em><span class="sig-paren">)</span> → classy_vision.optim.classy_optimizer.ClassyOptimizer<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.set_param_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the param schedulers for the Classy Optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param_schedulers</strong> – A dictionary of :class:<a href="#id1"><span class="problematic" id="id2">`</span></a>ClassyParamScheduler`s containing
the parameter scheduler to use for every parameter.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure: Optional[Callable] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<p>See <a href="#id3"><span class="problematic" id="id4">`</span></a>torch.optim.Optimizer.step &lt;<a class="reference external" href="https://pytorch.org/docs/stable/">https://pytorch.org/docs/stable/</a>
optim.html#torch.optim.Optimizer.step&gt;`_for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> – A closure that re-evaluates the model and returns the loss</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.update_schedule_on_epoch">
<code class="sig-name descname">update_schedule_on_epoch</code><span class="sig-paren">(</span><em class="sig-param">where: float</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.update_schedule_on_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the param schedule at the end of an epoch.</p>
<p>This should be called by the task at the end of every epoch to update the
schedule of epoch based param schedulers (See
<a class="reference internal" href="param_scheduler.html#classy_vision.optim.param_scheduler.ClassyParamScheduler" title="classy_vision.optim.param_scheduler.ClassyParamScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">param_scheduler.ClassyParamScheduler</span></code></a> for more information).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>where</strong> – where we are in terms of training progress (output of
<code class="xref py py-func docutils literal notranslate"><span class="pre">tasks.ClassyTask.where()</span></code>)</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.update_schedule_on_step">
<code class="sig-name descname">update_schedule_on_step</code><span class="sig-paren">(</span><em class="sig-param">where: float</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.update_schedule_on_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the param schedule at the end of a train step.</p>
<p>This should be called by the task at the end of every train step (
<code class="xref py py-func docutils literal notranslate"><span class="pre">tasks.ClassyTask.train_step()</span></code>) to update the schedule of step
based param schedulers (See <a class="reference internal" href="param_scheduler.html#classy_vision.optim.param_scheduler.ClassyParamScheduler" title="classy_vision.optim.param_scheduler.ClassyParamScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">param_scheduler.ClassyParamScheduler</span></code></a>
for more information).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>where</strong> – where we are in terms of training progress (output of
<a href="#id5"><span class="problematic" id="id6">:method:`ClassyTask.where`</span></a>)</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.ClassyOptimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.ClassyOptimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears the gradients of all optimized parameters.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad">torch.optim.Optimizer.zero_grad</a> for more information.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="classy_vision.optim.RMSProp">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">RMSProp</code><span class="sig-paren">(</span><em class="sig-param">lr: float = 0.1</em>, <em class="sig-param">momentum: float = 0</em>, <em class="sig-param">weight_decay: float = 0</em>, <em class="sig-param">alpha: float = 0.99</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">centered: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.RMSProp" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="classy_vision.optim.RMSProp.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">lr: float = 0.1</em>, <em class="sig-param">momentum: float = 0</em>, <em class="sig-param">weight_decay: float = 0</em>, <em class="sig-param">alpha: float = 0.99</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">centered: bool = False</em><span class="sig-paren">)</span> → None<a class="headerlink" href="#classy_vision.optim.RMSProp.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for ClassyOptimizer.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.RMSProp.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: Dict[str, Any]</em><span class="sig-paren">)</span> → classy_vision.optim.rmsprop.RMSProp<a class="headerlink" href="#classy_vision.optim.RMSProp.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a RMSProp from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> – A configuration for a RMSProp.
See <a class="reference internal" href="#classy_vision.optim.RMSProp.__init__" title="classy_vision.optim.RMSProp.__init__"><code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code></a> for parameters expected in the config.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A RMSProp instance.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.RMSProp.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.RMSProp.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.4.0a0+ac9da29 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to</p>
<p><code class="docutils literal notranslate"><span class="pre">super().init_pytorch_optimizer()</span></code></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="classy_vision.optim.SGD">
<em class="property">class </em><code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">SGD</code><span class="sig-paren">(</span><em class="sig-param">lr: float = 0.1</em>, <em class="sig-param">momentum: float = 0.0</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">nesterov: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.SGD" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="classy_vision.optim.SGD.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">lr: float = 0.1</em>, <em class="sig-param">momentum: float = 0.0</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">nesterov: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.SGD.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for ClassyOptimizer.</p>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.SGD.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: Dict[str, Any]</em><span class="sig-paren">)</span> → classy_vision.optim.sgd.SGD<a class="headerlink" href="#classy_vision.optim.SGD.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a SGD from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> – A configuration for a SGD.
See <a class="reference internal" href="#classy_vision.optim.SGD.__init__" title="classy_vision.optim.SGD.__init__"><code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code></a> for parameters expected in the config.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A SGD instance.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="classy_vision.optim.SGD.init_pytorch_optimizer">
<code class="sig-name descname">init_pytorch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.SGD.init_pytorch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the underlying <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch vmaster (1.4.0a0+ac9da29 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> instance.</p>
<p>Using the provided model, create param groups for the optimizer with a
weight decay override for params which should be left unregularized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deriving classes should initialize the underlying Pytorch optimizer
in this call. The simplest way to do this after a call to</p>
<p><code class="docutils literal notranslate"><span class="pre">super().init_pytorch_optimizer()</span></code></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This should called only after the model has been moved to the correct
device.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="function">
<dt id="classy_vision.optim.build_optimizer">
<code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">build_optimizer</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.build_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a ClassyOptimizer from a config.</p>
<p>This assumes a ‘name’ key in the config which is used to determine what
optimizer class to instantiate. For instance, a config <cite>{“name”: “my_optimizer”,
“foo”: “bar”}</cite> will find a class that was registered as “my_optimizer”
(see <a class="reference internal" href="#classy_vision.optim.register_optimizer" title="classy_vision.optim.register_optimizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_optimizer()</span></code></a>) and call .from_config on it.</p>
<p>Also builds the param schedulers passed in the config and associates them with the
optimizer. The config should contain an optional “param_schedulers” key containing a
dictionary of param scheduler configs, keyed by the parameter they control. Adds
“num_epochs” to each of the scheduler configs and then calls
<code class="xref py py-func docutils literal notranslate"><span class="pre">build_param_scheduler()</span></code> on each config in the dictionary.</p>
</dd></dl>
<dl class="function">
<dt id="classy_vision.optim.register_optimizer">
<code class="sig-prename descclassname">classy_vision.optim.</code><code class="sig-name descname">register_optimizer</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="headerlink" href="#classy_vision.optim.register_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a ClassyOptimizer subclass.</p>
<p>This decorator allows Classy Vision to instantiate a subclass of
ClassyOptimizer from a configuration file, even if the class itself is not
part of the Classy Vision framework. To use it, apply this decorator to a
ClassyOptimizer subclass, like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_optimizer</span><span class="p">(</span><span class="s1">'my_optimizer'</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MyOptimizer</span><span class="p">(</span><span class="n">ClassyOptimizer</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>To instantiate an optimizer from a configuration file, see
<a class="reference internal" href="#classy_vision.optim.build_optimizer" title="classy_vision.optim.build_optimizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">build_optimizer()</span></code></a>.</p>
</dd></dl>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Classy Vision</a></h1>
<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="heads.html">Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="hooks.html">Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="meters.html">Meters</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="param_scheduler.html">Param Schedulers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tasks.html">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Transforms</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="index.html">Documentation overview</a><ul>
<li>Previous: <a href="models.html" title="previous chapter">Models</a></li>
<li>Next: <a href="param_scheduler.html" title="next chapter">Param Schedulers</a></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><h5>Documentation</h5><a href="/tutorials/">Tutorials</a><a href="/api/">API Reference</a></div><div class="footerSection"><h5>Social</h5><div class="social"><a class="github-button" href="https://github.com/facebookresearch/ClassyVision" data-count-href="https://github.com/facebookresearch/ClassyVision/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star Classy Vision on GitHub">ClassyVision</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright"><span>Copyright © 2020 Facebook Inc.</span> </section><script>
            (function() {
              var BAD_BASE = '/classyvision/';
              if (window.location.origin !== 'https://classyvision.ai') {
                var pathname = window.location.pathname;
                var newPathname = pathname.slice(pathname.indexOf(BAD_BASE) === 0 ? BAD_BASE.length : 1);
                var newLocation = 'https://classyvision.ai/' + newPathname;
                console.log('redirecting to ' + newLocation);
                window.location.href = newLocation;
              }
            })();
          </script></footer></div></body></html>