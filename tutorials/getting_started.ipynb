{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Classy Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classy Vision is an end-to-end framework for image and video classification. Classy Vision makes it easy to write and launch distributed training jobs without having to worry about checkpointing, tensorboard logging and other features. Classy Vision already comes with a few computer vision baselines implemented, making it easy to make comparisions.\n",
    "\n",
    "## What can Classy Vision do for me?\n",
    "\n",
    "Here's a few examples of what Classy Vision handles for you in a typical training setting: \n",
    "\n",
    " * Checkpointing\n",
    " * Tensorboard logging\n",
    " * Mixed-precision training\n",
    " * Configuration files\n",
    " * Distributed training\n",
    " * Metrics (e.g. top-5 accuracy)\n",
    " * Parameter scheduling\n",
    " * .... and much more!\n",
    "\n",
    "## What about torchvision?\n",
    "\n",
    "[`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html) is a PyTorch package with common datasets, transforms and models used in computer vision research. While it provides many useful reference implementations, `torchvision` is designed to be a lightweight library and leaves a lot of up to the user. Classy Vision is an opinionated library built on top of `torchvision` that provide an end-to-end solution. After using Classy Vision for a while you will realize many Classy Vision abstractions simply use `torchvision` equivalents underneath, but play nicely with other classy components.\n",
    "\n",
    "## Let's get started!\n",
    "\n",
    "To start, let's create a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Running classy project!\n",
      "INFO:root:\n",
      "    Successfully generated template project at '/Users/vini/fb/vreis/ClassyVision/tutorials/my-project'.\n",
      "    To get started, run:\n",
      "        $ cd my-project\n",
      "        $ ./classy_train.py --config templates/config.json\n"
     ]
    }
   ],
   "source": [
    "! classy-project my-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('my-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Generic convolutional network trainer.\n",
      "WARNING:root:tensorboardX not installed, skipping tensorboard hooks\n",
      "INFO:root:Done setting up distributed process_group with rank 0, world_size 1\n",
      "INFO:root:Using CPU\n",
      "WARNING:root:Model contains unsupported modules:\n",
      "            Could not compute FLOPs for model forward pass. Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vini/miniconda3/envs/cv_tutorials/lib/python3.6/site-packages/classy_vision/hooks/model_complexity_hook.py\", line 39, in on_start\n",
      "    input_shape=task.base_model.input_shape,\n",
      "  File \"/Users/vini/miniconda3/envs/cv_tutorials/lib/python3.6/site-packages/classy_vision/models/classy_model.py\", line 260, in input_shape\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "INFO:root:Number of parameters in model: 11689512\n",
      "INFO:root:Advancing phase\n",
      "INFO:root:Recreating data loader for new phase\n",
      "WARNING:root:CUDA unavailable: CUDA events are not logged.\n",
      "INFO:root:Local unsynced metric values:\n",
      "INFO:root:Rank: 0, train phase: 0, processed batches: 5\n",
      "train loss: 4.874550373851912, LR rate: 0.1\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 0.453125, 'top_5': 0.75}}\n",
      "INFO:root:Local unsynced metric values:\n",
      "INFO:root:Rank: 0, train phase: 0, processed batches: 10\n",
      "train loss: 2.437275186925956, LR rate: 0.1\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 0.7569444179534912, 'top_5': 0.8888888955116272}}\n",
      "INFO:root:Syncing meters on phase end...\n",
      "INFO:root:...meters synced\n",
      "INFO:root:End of phase metric values:\n",
      "INFO:root:Rank: 0, train phase: 0, processed batches: 10\n",
      "train loss: 2.437275186925956, LR rate: 0.1\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 0.78125, 'top_5': 0.8999999761581421}}\n",
      "INFO:root:Average train batch time (ms) for 10 batches: 6921\n",
      "INFO:root:Train step time breakdown (rank 0):\n",
      "               Timer     Host    CudaEvent\n",
      "         read_sample:   25.30 ms    0.00 ms\n",
      "             forward: 2391.30 ms    0.00 ms\n",
      "      loss_allreduce:    0.04 ms    0.00 ms\n",
      "       meters_update:    0.63 ms    0.00 ms\n",
      "            backward: 4470.91 ms    0.00 ms\n",
      "      optimizer_step:   30.55 ms    0.00 ms\n",
      "    train_step_total: 6920.15 ms    0.00 ms\n",
      "INFO:root:Advancing phase\n",
      "INFO:root:Recreating data loader for new phase\n",
      "WARNING:root:CUDA unavailable: CUDA events are not logged.\n",
      "INFO:root:Syncing meters on phase end...\n",
      "INFO:root:...meters synced\n",
      "INFO:root:End of phase metric values:\n",
      "INFO:root:Rank: 0, test phase: 0, processed batches: 4\n",
      "test loss: 1209.1631469726562, LR rate: 0.1\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 0.4699999988079071, 'top_5': 1.0}}\n",
      "INFO:root:Average test batch time (ms) for 4 batches: 1619\n",
      "INFO:root:Advancing phase\n",
      "INFO:root:Recreating data loader for new phase\n",
      "WARNING:root:CUDA unavailable: CUDA events are not logged.\n",
      "INFO:root:Local unsynced metric values:\n",
      "INFO:root:Rank: 0, train phase: 1, processed batches: 5\n",
      "train loss: 0.0, LR rate: 0.01\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 1.0, 'top_5': 1.0}}\n",
      "INFO:root:Local unsynced metric values:\n",
      "INFO:root:Rank: 0, train phase: 1, processed batches: 10\n",
      "train loss: 0.0, LR rate: 0.01\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 1.0, 'top_5': 1.0}}\n",
      "INFO:root:Syncing meters on phase end...\n",
      "INFO:root:...meters synced\n",
      "INFO:root:End of phase metric values:\n",
      "INFO:root:Rank: 0, train phase: 1, processed batches: 10\n",
      "train loss: 0.0, LR rate: 0.01\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 1.0, 'top_5': 1.0}}\n",
      "INFO:root:Average train batch time (ms) for 10 batches: 6876\n",
      "INFO:root:Train step time breakdown (rank 0):\n",
      "               Timer     Host    CudaEvent\n",
      "         read_sample:   17.07 ms    0.00 ms\n",
      "             forward: 2355.16 ms    0.00 ms\n",
      "      loss_allreduce:    0.03 ms    0.00 ms\n",
      "       meters_update:    0.39 ms    0.00 ms\n",
      "            backward: 4468.59 ms    0.00 ms\n",
      "      optimizer_step:   32.51 ms    0.00 ms\n",
      "    train_step_total: 6874.31 ms    0.00 ms\n",
      "INFO:root:Advancing phase\n",
      "INFO:root:Recreating data loader for new phase\n",
      "WARNING:root:CUDA unavailable: CUDA events are not logged.\n",
      "INFO:root:Syncing meters on phase end...\n",
      "INFO:root:...meters synced\n",
      "INFO:root:End of phase metric values:\n",
      "INFO:root:Rank: 0, test phase: 1, processed batches: 4\n",
      "test loss: 21.604331016540527, LR rate: 0.01\n",
      "Meters:\n",
      "{'name': 'accuracy', 'value': {'top_1': 0.4699999988079071, 'top_5': 1.0}}\n",
      "INFO:root:Average test batch time (ms) for 4 batches: 1553\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!  ./classy_train.py --config configs/template_config.json --device cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've launched your first training run. This trained a small MLP model on a dataset made of random noise, which is not that useful. The `classy-project` utility creates the scaffolding for you project, and you should modify it according to your needs. We'll learn how to customize your runs on the next few tutorials.\n",
    "\n",
    "Let's take a look at what classy-project has created for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "./classy_train.py\r\n",
      "./configs\r\n",
      "./configs/template_config.json\r\n",
      "./datasets\r\n",
      "./datasets/__init__.py\r\n",
      "./datasets/my_dataset.py\r\n",
      "./losses\r\n",
      "./losses/__init__.py\r\n",
      "./losses/my_loss.py\r\n",
      "./models\r\n",
      "./models/__init__.py\r\n",
      "./models/my_model.py\r\n"
     ]
    }
   ],
   "source": [
    "! find . | grep -v \\.pyc | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what each folder means:\n",
    "\n",
    " * `configs`: stores your experiment configurations. Keeping all your experiments as separate configuration files helps making your research reproducible;\n",
    " * `models`: code for your custom model architectures;\n",
    " * `losses`: code for your custom loss functions;\n",
    " * `datasets`: code for your custom datasets;\n",
    " * `classy_train.py`: script to execute a training job; This uses the Classy Vision library to configure the job and execute it, and you might change it according to your needs;\n",
    " * `template_config.json`: experiment configuration file. This file is read by `classy_train.py` to configure your training job and launch it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"name\": \"classification_task\",\r\n",
      "    \"num_epochs\": 2,\r\n",
      "    \"loss\": {\r\n",
      "        \"name\": \"my_loss\"\r\n",
      "    },\r\n",
      "    \"dataset\": {\r\n",
      "        \"train\": {\r\n",
      "            \"name\": \"my_dataset\",\r\n",
      "            \"split\": \"train\",\r\n",
      "            \"crop_size\": 224,\r\n",
      "            \"class_ratio\": 0.5,\r\n",
      "            \"num_samples\": 320,\r\n",
      "            \"seed\": 0,\r\n",
      "            \"batchsize_per_replica\": 32,\r\n",
      "            \"use_shuffle\": true\r\n",
      "        },\r\n",
      "        \"test\": {\r\n",
      "            \"name\": \"my_dataset\",\r\n",
      "            \"split\": \"val\",\r\n",
      "            \"crop_size\": 224,\r\n",
      "            \"class_ratio\": 0.5,\r\n",
      "            \"num_samples\": 100,\r\n",
      "            \"seed\": 1,\r\n",
      "            \"batchsize_per_replica\": 32,\r\n",
      "            \"use_shuffle\": false\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"meters\": {\r\n",
      "        \"accuracy\": {\r\n",
      "            \"topk\": [1, 5]\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model\": {\r\n",
      "        \"name\": \"my_model\"\r\n",
      "    },\r\n",
      "    \"optimizer\": {\r\n",
      "        \"name\": \"sgd\",\r\n",
      "        \"lr\": {\r\n",
      "            \"name\": \"step\",\r\n",
      "            \"values\": [0.1, 0.01]\r\n",
      "        },\r\n",
      "        \"weight_decay\": 1e-4,\r\n",
      "        \"momentum\": 0.9\r\n",
      "    }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! cat configs/template_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That file can be shared with other researchers whenever you want them to reproduce your experiments. We generate `json` files by default but YAML is also supported. Take a look at the [Hydra tutorial](TODO) for an example of how to use YAML files within Classy Vision.\n",
    "\n",
    "## Interactive development\n",
    "\n",
    "Training scripts and configuration files are useful for running large training jobs on a GPU cluster (see our [AWS tutorial](TODO)), but a lot of day-to-day work happens interactively within Jupyter notebooks. Let's take a look at how to do the same training run as before, but within Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classy_vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classy_vision.dataset.transforms import ApplyTransformToKey\n",
    "from torchvision.transforms import ToTensor\n",
    "from datasets.my_dataset import MyDataset\n",
    "from models.my_model import MyModel\n",
    "from losses.my_loss import MyLoss\n",
    "\n",
    "train_dataset = MyDataset(\n",
    "    batchsize_per_replica=32,\n",
    "    shuffle=False,\n",
    "    transform=ApplyTransformToKey(ToTensor()),\n",
    "    num_samples=100,\n",
    "    crop_size=224,\n",
    "    class_ratio=0.5,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "test_dataset = MyDataset(\n",
    "    batchsize_per_replica=32,\n",
    "    shuffle=False,\n",
    "    transform=ApplyTransformToKey(ToTensor()),\n",
    "    num_samples=100,\n",
    "    crop_size=224,\n",
    "    class_ratio=0.5,\n",
    "    seed=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classy_vision.tasks import ClassificationTask\n",
    "from classy_vision.optim import SGD\n",
    "from classy_vision.optim.param_scheduler import ConstantParamScheduler\n",
    "\n",
    "model = MyModel()\n",
    "loss = MyLoss()\n",
    "\n",
    "# TODO: add defaults for momentum and weight decay\n",
    "optimizer = SGD(\n",
    "    lr_scheduler=ConstantParamScheduler(0.01), \n",
    "    momentum=0.99,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "from classy_vision.trainer import LocalTrainer\n",
    "\n",
    "task = ClassificationTask() \\\n",
    "        .set_model(model) \\\n",
    "        .set_dataset(train_dataset, \"train\") \\\n",
    "        .set_dataset(test_dataset, \"test\") \\\n",
    "        .set_loss(loss) \\\n",
    "        .set_optimizer(optimizer) \\\n",
    "        .set_num_epochs(1)\n",
    "\n",
    "trainer = LocalTrainer()\n",
    "trainer.train(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Your model is trained now and ready for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn((1, 3, 224, 224))\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "\n",
    "# TODO: use a different model here so you actually get scores back\n",
    "y_hat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
