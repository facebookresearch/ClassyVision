{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do video classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will show how to train a video classification model in Classy Vision. Given an input video, the video classification task is to predict the most probable class label. This is very similar to image classification, which was covered in other tutorials, but there are a few differences that make video special. As the video duration can be long, we sample short video clips of a small number of frames, use the classifier to make predictions, and finally average the clip-level predictions to get the final video-level predictions. \n",
    "\n",
    "In this tutorial we will: (1) load a video dataset; (2) configure a video model; (3) configure video meters; (4) build a task; (5) start training; Please note that these steps are being done separately in the tutorial for easy of exposition in the notebook format. As described in our [Getting started](https://classyvision.ai/tutorials/getting_started) tutorial, you can combine all configs used in this tutorial into a single config for ClassificationTask and train it using `classy_train.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the dataset\n",
    "\n",
    "All right! Let's start with the dataset. [UCF-101](https://www.crcv.ucf.edu/data/UCF101.php) is a canonical action recognition dataset. It has 101 action classes, and has 3 folds with different training/testing splitting . We use fold 1 in this tutorial. Classy Vision has implemented the dataset `ucf101`, which can be used to load the training and testing splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classy_vision.dataset import build_dataset\n",
    "\n",
    "# set it to the folder where video files are saved\n",
    "video_dir = \"[PUT YOUR VIDEO FOLDER HERE]\"\n",
    "# set it to the folder where dataset splitting files are saved\n",
    "splits_dir = \"[PUT THE FOLDER WHICH CONTAINS SPLITTING FILES HERE]\"\n",
    "# set it to the file path for saving the metadata\n",
    "metadata_file = \"[PUT THE FILE PATH OF DATASET META DATA HERE]\"\n",
    "\n",
    "datasets = {}\n",
    "datasets[\"train\"] = build_dataset({\n",
    "    \"name\": \"ucf101\",\n",
    "    \"split\": \"train\",\n",
    "    \"batchsize_per_replica\": 8,  # For training, we use 8 clips in a minibatch in each model replica\n",
    "    \"use_shuffle\": True,         # We shuffle the clips in the training split\n",
    "    \"num_samples\": 64,           # We train on 16 clips in one training epoch\n",
    "    \"clips_per_video\": 1,        # For training, we randomly sample 1 clip from each video\n",
    "    \"frames_per_clip\": 8,        # The video clip contains 8 frames\n",
    "    \"video_dir\": video_dir,\n",
    "    \"splits_dir\": splits_dir,\n",
    "    \"metadata_file\": metadata_file,\n",
    "    \"fold\": 1,\n",
    "    \"transforms\": {\n",
    "        \"video\": [\n",
    "            {\n",
    "                \"name\": \"video_default_augment\",\n",
    "                \"crop_size\": 112,\n",
    "                \"size_range\": [128, 160]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "})\n",
    "datasets[\"test\"] = build_dataset({\n",
    "    \"name\": \"ucf101\",\n",
    "    \"split\": \"test\",\n",
    "    \"batchsize_per_replica\": 10,  # For testing, we will take 1 video once a time, and sample 10 clips per video\n",
    "    \"use_shuffle\": False,         # We do not shuffle clips in the testing split\n",
    "    \"num_samples\": 80,            # We test on 80 clips in one testing epoch\n",
    "    \"clips_per_video\": 10,        # We sample 10 clips per video\n",
    "    \"frames_per_clip\": 8,\n",
    "    \"video_dir\": video_dir,\n",
    "    \"splits_dir\": splits_dir,\n",
    "    \"metadata_file\": metadata_file,\n",
    "    \"fold\": 1,\n",
    "    \"transforms\": {\n",
    "        \"video\": [\n",
    "            {\n",
    "                \"name\": \"video_default_no_augment\",\n",
    "                \"size\": 128\n",
    "            }\n",
    "        ]\n",
    "    }    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we specify different transforms for training and testing split. For training split, we first randomly select a size from `size_range` [128, 160], and resize the video clip so that its short edge is equal to the random size. After that, we take a random crop of spatial size 112 x 112. We find such data augmentation helps the model generalize better, and use it as the default transform with data augmentation. For testing split, we resize the video clip to have short edge of size 128, and skip the random cropping to use the entire video clip. This is the default transform without data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a model trunk and a head\n",
    "\n",
    "Next, let's create the video model, which consists of a trunk and a head. The trunk can be viewed as a feature extractor for computing discriminative features from raw video pixels while the head is viewed as a classifier for producing the final predictions. Let's first create the trunk of architecture ResNet3D-18 by using the built-in `resnext3d` model in Classy Vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classy_vision.models import build_model\n",
    "\n",
    "model = build_model({\n",
    "    \"name\": \"resnext3d\",\n",
    "    \"frames_per_clip\": 8,        # The number of frames we have in each video clip\n",
    "    \"input_planes\": 3,           # We use RGB video frames. So the input planes is 3\n",
    "    \"clip_crop_size\": 112,       # We take croppings of size 112 x 112 from the video frames \n",
    "    \"skip_transformation_type\": \"postactivated_shortcut\",    # The type of skip connection in residual unit\n",
    "    \"residual_transformation_type\": \"basic_transformation\",  # The type of residual connection in residual unit\n",
    "    \"num_blocks\": [2, 2, 2, 2],  # The number of residual blocks in each of the 4 stages \n",
    "    \"input_key\": \"video\",        # The key used to index into the model input of dict type \n",
    "    \"stage_planes\": 64,    \n",
    "    \"num_classes\": 101           # the number of classes\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create a model head, which consists of an average pooling layer and a linear layer, by using the `fully_convolutional_linear` head. At test time, the shape (channels, frames, height, width) of input tensor is typically `(3 x 8 x 128 x 173)`. The shape of input tensor to the average pooling layer is `(2048, 1, 8, 10)`. Since we do not use a global average pooling but an average pooling layer of kernel size `(1, 7, 7)`, the pooled feature map has shape `(2048, 1, 2, 5)`. The shape of prediction tensor from the linear layer is `(1, 2, 5, 101)`, which indicates the model computes a 101-D prediction vector densely over a `2 x 5` grid. That's why we name the head as `FullyConvolutionalLinearHead` because we use the linear layer as a `1x1` convolution layer to produce spatially dense predictions. Finally, predictions over the `2 x 5` grid are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classy_vision.heads import build_head\n",
    "from collections import defaultdict\n",
    "\n",
    "unique_id = \"default_head\"\n",
    "head = build_head({\n",
    "    \"name\": \"fully_convolutional_linear\",\n",
    "    \"unique_id\": unique_id,\n",
    "    \"pool_size\": [1, 7, 7],\n",
    "    \"num_classes\": 101,\n",
    "    \"in_plane\": 512    \n",
    "})\n",
    "# In Classy Vision, the head can be attached to any residual block in the trunk. \n",
    "# Here we attach the head to the last block as in the standard ResNet model\n",
    "fork_block = \"pathway0-stage4-block1\"\n",
    "heads = defaultdict(dict)\n",
    "heads[fork_block][unique_id] = head\n",
    "model.set_heads(heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Choose the meters\n",
    "\n",
    "This is the biggest difference between video and image classification. For images we used `AccuracyMeter` to measure top-1 and top-5 accuracy. For videos you can also use both `AccuracyMeter` and `VideoAccuracyMeter`, but they behave differently:\n",
    " * `AccuracyMeter` takes one clip-level prediction and compare it with groundtruth video label. It reports the clip-level accuracy.\n",
    " * `VideoAccuracyMeter` takes multiple clip-level predictions from the same video, averages them and compares that with groundtruth video label. It reports the video-level accuracy which is usually higher than clip-level accuracy. \n",
    " \n",
    " Both meters report top-1 and top-5 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classy_vision.meters import build_meters, AccuracyMeter, VideoAccuracyMeter\n",
    "\n",
    "meters = build_meters({\n",
    "    \"accuracy\": {\n",
    "        \"topk\": [1, 5]\n",
    "    },\n",
    "    \"video_accuracy\": {\n",
    "        \"topk\": [1, 5],\n",
    "        \"clips_per_video_train\": 1,\n",
    "        \"clips_per_video_test\": 10\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build a task\n",
    "Great! we have defined the minimal set of components necessary for video classification, including dataset, model, loss function, meters and optimizer. We proceed to define a video classification task, and populate it with all the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classy_vision.tasks import ClassificationTask\n",
    "from classy_vision.optim import build_optimizer\n",
    "from classy_vision.losses import build_loss\n",
    "\n",
    "loss = build_loss({\"name\": \"CrossEntropyLoss\"})\n",
    "\n",
    "optimizer = build_optimizer({\n",
    "    \"name\": \"sgd\",\n",
    "    \"param_schedulers\": {\n",
    "        \"lr\": {\n",
    "            \"name\": \"multistep\",\n",
    "            \"values\": [0.005, 0.0005],\n",
    "            \"milestones\": [1]\n",
    "        }\n",
    "    },\n",
    "    \"num_epochs\": 2,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"momentum\": 0.9\n",
    "})\n",
    "\n",
    "num_epochs = 2\n",
    "task = (\n",
    "    ClassificationTask()\n",
    "    .set_num_epochs(num_epochs)\n",
    "    .set_loss(loss)\n",
    "    .set_model(model)\n",
    "    .set_optimizer(optimizer)\n",
    "    .set_meters(meters)\n",
    ") \n",
    "for phase in [\"train\", \"test\"]:\n",
    "    task.set_dataset(datasets[phase], phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Start training\n",
    "\n",
    "After creating a task, you can simply pass that to a Trainer to start training. Here we will train on a single node and \n",
    "configure logging and checkpoints for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "from classy_vision.trainer import LocalTrainer\n",
    "from classy_vision.hooks import CheckpointHook\n",
    "from classy_vision.hooks import LossLrMeterLoggingHook\n",
    "\n",
    "hooks = [LossLrMeterLoggingHook(log_freq=4)]\n",
    "\n",
    "checkpoint_dir = f\"/tmp/classy_checkpoint_{time.time()}\"\n",
    "os.mkdir(checkpoint_dir)\n",
    "hooks.append(CheckpointHook(checkpoint_dir, input_args={}))\n",
    "\n",
    "task = task.set_hooks(hooks)\n",
    "\n",
    "trainer = LocalTrainer()\n",
    "trainer.train(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the training progresses, you should see `LossLrMeterLoggingHook` printing the loss, learning rate and meter metrics. Checkpoints will be available in the folder created above.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "Video classification is very similar to image classification in Classy Vision, you just need to use an appropriate dataset, model and meters. This tutorial glossed over many details about training, please take a look at our [Getting started](https://classyvision.ai/tutorials/getting_started) tutorial to learn more. Refer to our API reference for more details about [ResNeXt3D](https://classyvision.ai/api/models.html#classy_vision.models.ResNeXt3D) models, [UCF101](https://classyvision.ai/api/dataset.html#classy_vision.dataset.UCF101Dataset) dataset and [VideoAccuracy](http://classyvision.ai/api/meters.html#classy_vision.meters.VideoAccuracyMeter) meters.\n"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
